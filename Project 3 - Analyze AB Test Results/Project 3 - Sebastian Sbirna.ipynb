{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Analyze A/B Test Results\n",
    "\n",
    "**_By Sebastian Sbirna_**\n",
    "\n",
    "An e-commerce company has developed a new web page in order to try and increase the number of users who \"convert,\" meaning the number of users who decide to pay for the company's product. Through this notebook, the company's decision will be tested to let the company understand if they should implement this new page, keep the old page, or perhaps run the experiment longer to make their decision.\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)\n",
    "- [Conclusions](#conclusions)\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "A/B tests are very commonly performed by data analysts and data scientists to test changes on a web page by running an experiment where a control group sees the old version, while the experiment group sees the new version. A metric (_in our case, the \"converted\" rate_) is then chosen to measure the level of engagement from users in each group. These results are then used to judge whether one version is more effective than the other.\n",
    "\n",
    "This project will analyze the statistical likelihood of increasing conversion rates due to a newer version of the landing page, and will consolidate and validate its results by demonstrating the same statistical effects using three statistical methods: probability, hypothesis testing, and, lastly, logistic regression. \n",
    "\n",
    "<a id='probability'></a>\n",
    "#### Part I - Probability\n",
    "\n",
    "To get started, we're importing our necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are loading the dataset from our .csv file, and inspecting its format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ab_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it's a good idea to understand more about its contents before moving to data cleaning and exploration:\n",
    "\n",
    "First, we check for the shape of the DataFrame (showing the total nr. of elements) and the Python-inserted variable types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(294478, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id          int64\n",
       "timestamp       object\n",
       "group           object\n",
       "landing_page    object\n",
       "converted        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we look into the number of unique values for each column set, so as to detect possible inconsistencies or duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         290584\n",
       "timestamp       294478\n",
       "group                2\n",
       "landing_page         2\n",
       "converted            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From the number of unique values, we can identify that there are there are as many unique timestamps as there are entries, however the number of users is less than the number of entries (even though the data collection process was set-up in a way that this should not have happened).\n",
    "\n",
    "We want to find out the number of inconsistencies, which are likely due to mismatching between the group the user was randomly placed in and the version of the landing page received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3893"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[(df['group'] == 'treatment') & (df['landing_page'] != 'new_page')]) + \\\n",
    "len(df[(df['group'] == 'control') & (df['landing_page'] != 'old_page')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, there are such inconsistencies, which we will eliminate straight away.\n",
    "\n",
    "We will check to see if any of the rows have missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         False\n",
       "timestamp       False\n",
       "group           False\n",
       "landing_page    False\n",
       "converted       False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing or NaN values, meaning we can proceed into eliminating the mismatched entries.\n",
    "\n",
    "For the rows where `treatment` is not aligned with `new_page` or `control` is not aligned with `old_page`, we cannot be sure if these users truly received the new or old page. Since we should only use the entries that we can feel confident in the accuracy of the data, the mismatched rows will be removed (_dropped_)\n",
    "\n",
    "We will create a new dataset that meets these specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop(df.query(\"group == 'control' and landing_page == 'new_page'\").index)\\\n",
    "        .drop(df.query(\"group == 'treatment' and landing_page == 'old_page'\").index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Double Check all of the correct rows were removed - this should be 0\n",
    "len(df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check again for unique values, to see if the number of unique users align with the number of total entries of the dataset (_as each user should have only been recorded once, belonging into either the_ `control` _or the_ `treatment` _group_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         290584\n",
       "timestamp       290585\n",
       "group                2\n",
       "landing_page         2\n",
       "converted            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There is one `user_id` which is still repeated in `df2`. We will investigate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773192\n"
     ]
    }
   ],
   "source": [
    "print(df2.user_id.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will check the entries for this user, in order to understand why it is a duplicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2.user_id == df2.user_id.value_counts().index[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears to have been some sort of error within the data collection process, as these entries are neither mismatched in group or landing_page nor in conversion status.\n",
    "\n",
    "In this case, by analysing both entries, it is safe to say that the only difference between them is the timestamp of collection. We will therefore remove one of the duplicate rows, at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.drop_duplicates('user_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned our dataset, we can start the statistical analysis of the impact of the two page formats upon the conversion rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute some essential probabilities which will help us understand the task better, starting with _the probability of an individual converting regardless of the page they receive_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, given that an individual was in the `control` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_obs_control = df2.query(\"group == 'control'\").converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, given that an individual was in the `treatment` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_obs_treatment = df2.query(\"group == 'treatment'\").converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can compute the observed difference between conversion probabilites from our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0015782389853555567"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_value = p_obs_treatment - p_obs_control\n",
    "observed_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this A/B test is an experimental test, we would like to uniformly distribute the users in a _**random**_ manner into the `control` and the `treatment` group respectively. \n",
    "\n",
    "We will check for the distribution of the `landing_page`, just to check whether it comes close to ~0.5 (50%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000619442226688"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2.query(\"landing_page == 'new_page'\")) / len(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_So far, based on the computed probability of conversion, we would be inclined to say that there is not enough evidence to suggest that the new webpage design increases in any way conversion rate. And, looking at the magnitude of the difference, it seems clear to say that this result is not practically significant either._** \n",
    "\n",
    "**_However, even though we would be inclined to consider this result as significant due to the large sample size of the test, it needs to be noticed that our previous calculations were only describing the observed probabilities, however we have mentioned nothing about the statistical significance of such results._**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### Part II - A/B Test\n",
    "\n",
    "In order to provide reasoning behind our choices, we will want to verify the results from the observed values in a statistical manner, which can determine with enough confidence whether or not the new webpage design should be implemented.\n",
    "\n",
    "We will now perform a statistical hypothesis test upon the conversion rate of the two page versions.\n",
    "\n",
    "We assume that the old page is better unless the new page proves to be definitely better at a Type I error rate ($\\alpha$) of _5%_. \n",
    "\n",
    "This translates into the following hypotheses, where **$p_{old}$** and **$p_{new}$** are the converted rates for the old and new pages:\n",
    "\n",
    "$$ H_0: p_{new} \\leq p_{old} \\Leftrightarrow H_0: p_{new} - p_{old} \\leq 0 $$\n",
    "$$ H_1: p_{new} > p_{old} \\Leftrightarrow H_1: p_{new} - p_{old} > 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume under the null hypothesis that $p_{new}$ and $p_{old}$ both have success rates equal to the `converted` success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal, and they are equal to the `converted` rate in our dataset, regardless of the page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the null hypothesis, we will store the value of $p_{new}$ and $p_{old}$ (which is the same, due to the being under the null hypothesis) for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_new_null = df2.converted.mean()\n",
    "p_new_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_old_null = df2.converted.mean()\n",
    "p_old_null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we will compute the number of users having received each of the page versions in $n_{new}$ and $n_{old}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145310"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_new = len(df2[df2.landing_page == 'new_page'])\n",
    "n_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145274"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_old = len(df2[df2.landing_page == 'old_page'])\n",
    "n_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will simulate 10,000 $p_{new}$ - $p_{old}$ values, in order to perform a bootstrapped sampling distribution for the difference in conversion rates between the two pages over 10,000 iterations of calculating an estimate from the null. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_page_converted = np.random.binomial(n_new, p_new_null, 10000) / n_new\n",
    "old_page_converted = np.random.binomial(n_old, p_old_null, 10000) / n_old\n",
    "\n",
    "p_diffs = new_page_converted - old_page_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A histogram of the bootstrapped sampling distribution from the null is presented below. We will place a red line where the `observed_value` would fit into our distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x266314e4f98>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(p_diffs)\n",
    "plt.axvline(obs_value, color = \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proportion of the `p_diffs` which are greater than the actual difference observed in our dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9046"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value = (p_diffs > obs_value).mean()\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*Here we have computed the statistical probability of getting a result such as the observed value (obs_value) while being under the null hypothesis. We have previously set the significance level of this hypothesis test to $\\alpha = 0.05$. Therefore, any p-value above 0.05 (5%) would not suggest the rejection of the null hypothesis.*__\n",
    "\n",
    "**_Since our p-value is 0.9064 (~90%), this suggests that, with extremely high likelihood, the behaviour of our tested value (the difference in conversion ratios between old and new pages) is as per the null hypothesis, meaning that the old page conversion ratios are better (or at least equal) to those of the new page._**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also have used a built-in library to achieve similar results. Though using the built-in library might be easier to code, the above portions are a walkthrough of the ideas that are critical to thinking correctly about statistical significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Below is a run-down of the same hypothesis test using the z-test from Python statistical model library:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "convert_old = len(df2.query(\"landing_page == 'old_page'\").query(\"converted == 1\"))\n",
    "convert_new = len(df2.query(\"landing_page == 'new_page'\").query(\"converted == 1\"))\n",
    "n_old = len(df2.query(\"landing_page == 'old_page'\"))\n",
    "n_new = len(df2.query(\"landing_page == 'new_page'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.3109241984234394, 0.9050583127590245)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_score, p_value = sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative= 'larger')\n",
    "z_score, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*The z-score and p-value agree fully to the previous manual results. More precisely, we can see only a very slight difference in p-values between the two tests (0.906 vs. 0.905). The results show that, indeed, the old page generates better conversion rates than the new one.*__\n",
    "\n",
    "__*The z-score is representing how many standard deviations  our observed value (differences between proportions) is from the mean of the distribution (which is sampled from the null hypothesis). A z-score of -1.31 means that our observed value is $1.31\\sigma$ lower than the mean of the sampling distribution. Looking for the area under the bell curve to the right of the z-score value, for a graph of the sampling distribution, it means that our observed difference was lower in value than 90.4% of the random sampling differences.*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "### Part III - A regression approach\n",
    "\n",
    "In this final part, we will examine and re-evaluate the result acheived in the previous A/B test by performing logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*We are interested in determining whether individuals will convert or not, depending on the type of landing page seen.*__\n",
    "\n",
    "\n",
    "__*Therefore, we have a response variable characterized by categorical \"response - no response\" values. Since each row is either a conversion or no conversion, our regression approach indicates using a logistic regression in order to get the best prediction model possible.*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal here is to use `statsmodels` to fit the logistic regression model to see if there is a significant difference in conversion based on which page a customer receives.  \n",
    "\n",
    "First, we need to create a column for the intercept, and also create a dummy variable column for which page each user received.\n",
    "An `intercept` column will be added, as well as an `ab_page` column, which represents 1 when an individual receives the **treatment** and 0 for **control**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['intercept'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.join(pd.get_dummies(df['landing_page']))\n",
    "df2 = df2.drop('old_page', axis = 1)\n",
    "df2 = df2.rename({'new_page' : 'ab_page'}, axis = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **statsmodels** to fit our regression model using the `intercept` and `ab_page` columns to predict whether or not an individual converts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "model = sm.Logit(endog = df2['converted'], exog = df2[['intercept', 'ab_page']])\n",
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary of the logistic model is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:              converted   No. Observations:               290584\n",
      "Model:                          Logit   Df Residuals:                   290582\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Sat, 10 Nov 2018   Pseudo R-squ.:               8.077e-06\n",
      "Time:                        22:55:59   Log-Likelihood:            -1.0639e+05\n",
      "converged:                       True   LL-Null:                   -1.0639e+05\n",
      "                                        LLR p-value:                    0.1899\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept     -1.9888      0.008   -246.669      0.000      -2.005      -1.973\n",
      "ab_page       -0.0150      0.011     -1.311      0.190      -0.037       0.007\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*As we can see from the summary of our logistic regression model, the predictor variable (relating to the outcome of having recieved a new or old page version) is not shown to be significantly contributing to predicting conversion rate in any way, neither statistically nor practically. That is due to the high p-value obtained: 0.19. Low statistical significance of our predictor can also be demonstrated from the inclusion of the null-hypothesis value of 0 in the 95% confidence interval.*__\n",
    "\n",
    "__*For reference, we might be asking ourselves why this p-value differs to the one obtained in our A/B hypothesis testing. This is due to the different hypotheses chosen for the two tests. In the logistic regression case, our hypothesis for the predictor variable were chosen as :*__\n",
    "\n",
    "$$ H_0: \\beta_{ab\\_test} = 0 $$\n",
    "$$ H_1: \\beta_{ab\\_test} \\neq 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Generally, for a regression test, we would want to have as many as possible of linearly independent predictors related to our response variable* (`converted`)*, as this will increase the chances for the model to correctly determine which individuals are more likely to convert, and we will be able to find out which predictors are significant in increasing conversion.*\n",
    "\n",
    "*However, for our data, adding some of our remaining variables will actually decrease the accuracy of our model and introduce multicollinearity errors. This is because the variables* `group`, `landing_page` *and* `ab_page` *are strongly collinear, as they represent mainly the same thing: whether the individual has experiment conditions from control type or from treatment type.*\n",
    "\n",
    "*Moreover, adding the* `user_id` *to our model is likely irrelevant, as all of our users have only one dataset entry each. Timestamp of conversion could be interesting to consider, if we would transform it into a categorical variable representing e.g. the month during which conversion was made.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now along with testing if the conversion rate changes for different pages, we also add an effect based on which country a user lives in (either US, UK or Canada), due to the making available of another dataset: **countries.csv**, containing the country where the unique user (_determined by the_ `user_id`_)_ resides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df = pd.read_csv('./countries.csv')\n",
    "df_new = countries_df.set_index('user_id').join(df2.set_index('user_id'), how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new = df_new.join(pd.get_dummies(df_new['country']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366113\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "model = sm.Logit(endog = df_new['converted'], exog = df_new[['intercept', 'ab_page', 'CA', 'UK']])\n",
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:              converted   No. Observations:               290584\n",
      "Model:                          Logit   Df Residuals:                   290580\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Sat, 10 Nov 2018   Pseudo R-squ.:               2.323e-05\n",
      "Time:                        22:56:02   Log-Likelihood:            -1.0639e+05\n",
      "converged:                       True   LL-Null:                   -1.0639e+05\n",
      "                                        LLR p-value:                    0.1760\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept     -1.9893      0.009   -223.763      0.000      -2.007      -1.972\n",
      "ab_page       -0.0149      0.011     -1.307      0.191      -0.037       0.007\n",
      "CA            -0.0408      0.027     -1.516      0.130      -0.093       0.012\n",
      "UK             0.0099      0.013      0.743      0.457      -0.016       0.036\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___By adding the catergorical predictor_ `country`__, ___we are trying to discover whether it is statistically significant that our users come from the US, UK or Canada.___\n",
    "\n",
    "___Considering the large p-values of___ **0.13 and 0.457**,  ***respectively, for the predictors of having users from CA and UK, we can understand that, regardless on whether our users come from the baseline category (US) or any of the other possible countries, the conversion rate for these users will not increase much.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the past two models, we have only looked at the influence of the individual factors of `country` and `ab_page` on conversion.\n",
    "\n",
    "Now, we would also like to look at a possible interaction between `ab_page` and `country` to see if there are significant effects on conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['CA_page_ab'] = df_new['CA'] * df_new['ab_page']\n",
    "df_new['UK_page_ab'] = df_new['UK'] * df_new['ab_page']\n",
    "df_new['US_page_ab'] = df_new['US'] * df_new['ab_page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "model = sm.Logit(endog = df_new['converted'], exog = df_new[['intercept', 'ab_page', 'UK', 'CA', 'UK_page_ab', 'CA_page_ab']])\n",
    "model_fit = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:              converted   No. Observations:               290584\n",
      "Model:                          Logit   Df Residuals:                   290578\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Sat, 10 Nov 2018   Pseudo R-squ.:               3.482e-05\n",
      "Time:                        22:56:04   Log-Likelihood:            -1.0639e+05\n",
      "converged:                       True   LL-Null:                   -1.0639e+05\n",
      "                                        LLR p-value:                    0.1920\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "intercept     -1.9865      0.010   -206.344      0.000      -2.005      -1.968\n",
      "ab_page       -0.0206      0.014     -1.505      0.132      -0.047       0.006\n",
      "UK            -0.0057      0.019     -0.306      0.760      -0.043       0.031\n",
      "CA            -0.0175      0.038     -0.465      0.642      -0.091       0.056\n",
      "UK_page_ab     0.0314      0.027      1.181      0.238      -0.021       0.084\n",
      "CA_page_ab    -0.0469      0.054     -0.872      0.383      -0.152       0.059\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_As before, the p-values for the predictors in this model do not show any statistical significance. Therefore, it is unlikely that an interaction between the landing page version and the user country will influence our conversion rates._**\n",
    "\n",
    "<a id='conclusions'></a>\n",
    "## Conclusions\n",
    "\n",
    "**_As an overall conclusion, we have been able to demonstrate statistically, using either of: probabilistic reasoning, hypothesis testing, and logistic regression, that:_** \n",
    "* **_receiving the newer version of the landing page will not increase user conversion rate for the e-commerce company;_**\n",
    "* **_demographic distribution of users also is statistically unlikely to influence user conversion rate for the e-commerce company;_**\n",
    "\n",
    "**_Overall, we have noticed neither statistical nor practical significance in the company launching the new page version._**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
